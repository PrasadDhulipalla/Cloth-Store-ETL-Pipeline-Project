from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit
import psycopg2
import traceback

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("S3ToRedshift") \
    .config("spark.jars", "/dbfs/FileStore/jars/redshift_jdbc42_2_1_0_30.jar") \
    .getOrCreate()

# Redshift Configurations
redshift_jdbc_url = "jdbc:redshift://databricks1.535002865035.eu-west-2.redshift-serverless.amazonaws.com:5439/dev"
redshift_user = "prasad"
redshift_password = "Welcomedp*123456"
redshift_properties = {
    "user": redshift_user,
    "password": redshift_password,
    "driver": "com.amazon.redshift.jdbc42.Driver",
}

# Function to create table in Redshift
def create_table_in_redshift(schema, table_name, jdbc_url, user, password):
    try:
        conn = psycopg2.connect(
            dbname="dev",
            user=user,
            password=password,
            host="databricks1.535002865035.eu-west-2.redshift-serverless.amazonaws.com",
            port=5439
        )
        cursor = conn.cursor()
        cursor.execute(schema)
        conn.commit()
        print(f"Table '{table_name}' created successfully in Redshift.")
    except Exception as e:
        print(f"ERROR: Failed to create table '{table_name}' in Redshift.")
        print(f"Exception: {str(e)}")
        traceback.print_exc()
    finally:
        if 'cursor' in locals():
            cursor.close()
        if 'conn' in locals():
            conn.close()

# SQL Queries for Upsert and Cleanup
def perform_upsert_and_cleanup(stage_table, target_table, jdbc_url, user, password):
    try:
        conn = psycopg2.connect(
            dbname="dev",
            user=user,
            password=password,
            host="databricks1.535002865035.eu-west-2.redshift-serverless.amazonaws.com",
            port=5439
        )
        cursor = conn.cursor()

        # Upsert Logic: Update Existing Records
        update_query = f"""
        UPDATE {target_table} AS target
        SET
            Name = stage.Name,
            Email = stage.Email,
            Phone = stage.Phone,
            Gender = stage.Gender,
            DateOfBirth = stage.DateOfBirth,
            LoyaltyPoints = stage.LoyaltyPoints,
            FavoriteStore = stage.FavoriteStore,
            FavoriteCategory = stage.FavoriteCategory
        FROM {stage_table} AS stage
        WHERE target.CustomerID = stage.CustomerID;
        """
        cursor.execute(update_query)
        conn.commit()
        print("Update query executed successfully.")

        # Insert Logic: Insert New Records
        insert_query = f"""
        INSERT INTO {target_table} (
            CustomerID, Name, Email, Phone, Gender, DateOfBirth,
            LoyaltyPoints, FavoriteStore, FavoriteCategory
        )
        SELECT
            CustomerID, Name, Email, Phone, Gender, DateOfBirth,
            LoyaltyPoints, FavoriteStore, FavoriteCategory
        FROM {stage_table}
        WHERE CustomerID NOT IN (SELECT CustomerID FROM {target_table});
        """
        cursor.execute(insert_query)
        conn.commit()
        print("Insert query executed successfully.")

        # Drop Staging Table
        drop_query = f"DROP TABLE IF EXISTS {stage_table};"
        cursor.execute(drop_query)
        conn.commit()
        print(f"Staging table '{stage_table}' dropped successfully.")

    except Exception as e:
        print(f"ERROR: Failed to perform upsert and cleanup for '{stage_table}'.")
        print(f"Exception: {str(e)}")
        traceback.print_exc()
    finally:
        if 'cursor' in locals():
            cursor.close()
        if 'conn' in locals():
            conn.close()

# Function for Each Table
def create_and_process_table(schema, stage_table, target_table, s3_path, jdbc_url, user, password):
    try:
        # Read data from S3
        df = spark.read \
            .format("csv") \
            .option("header", "true") \
            .option("inferSchema", "true") \
            .load(s3_path)

        # Cast all columns as string to avoid scientific notation issues
        df = df.select([col(c).cast("string").alias(c) for c in df.columns])

        # Create Staging Table in Redshift
        create_table_in_redshift(schema, stage_table, jdbc_url, user, password)

        # Write data to Staging Table in Redshift
        df.write \
            .format("jdbc") \
            .option("url", jdbc_url) \
            .option("dbtable", stage_table) \
            .option("user", user) \
            .option("password", password) \
            .option("driver", redshift_properties["driver"]) \
            .mode("append") \
            .save()

        print(f"Data from {s3_path} written to staging table '{stage_table}'.")

        # Perform Upsert and Cleanup
        perform_upsert_and_cleanup(stage_table, target_table, jdbc_url, user, password)

    except Exception as e:
        print(f"ERROR: Failed to process data for '{stage_table}'.")
        print(f"Exception: {str(e)}")
        traceback.print_exc()

# Table Definitions and Schema
table_schemas = {
    "Dim_Employee": """
        CREATE TABLE IF NOT EXISTS stage_Employee (
            CustomerID VARCHAR(255) PRIMARY KEY,
            Email VARCHAR(255),
            Phone VARCHAR(50),
            Gender VARCHAR(50),
            DateOfBirth DATE,
            LoyaltyPoints INTEGER,
            FavoriteStore VARCHAR(255),
            FavoriteCategory VARCHAR(255)
        );
    """,
    "Dim_Products": """
        CREATE TABLE IF NOT EXISTS stage_Products (
            ProductID VARCHAR(25) PRIMARY KEY,
            ProductName VARCHAR(255),
            Category VARCHAR(255),
            Brand VARCHAR(255),
            Price DOUBLE PRECISION,
            DiscountedPrice DOUBLE PRECISION,
            Gender VARCHAR(50),
            Size VARCHAR(50),
            Color VARCHAR(50),
            StockQuantity INTEGER,
            WarehouseLocation VARCHAR(255),
            DateAdded DATE
        );
    """,
    "Dim_WebData": """
        CREATE TABLE IF NOT EXISTS stage_WebData (
            SessionID VARCHAR(255),
            CustomerID VARCHAR(25) PRIMARY KEY,
            PageViewed VARCHAR(255),
            Duration INTEGER,
            Date DATE,
            Device VARCHAR(255),
            Referrer VARCHAR(255)
        );
    """,
    "Dim_Store": """
        CREATE TABLE IF NOT EXISTS stage_Store (
            StoreID VARCHAR(25) PRIMARY KEY,
            StoreName VARCHAR(255),
            Address VARCHAR(255),
            City VARCHAR(255),
            State VARCHAR(255),
            Country VARCHAR(255),
            Region VARCHAR(255),
            OpeningDate DATE,
            ManagerID VARCHAR(255)
        );
    """,
    "Fact_Sales": """
        CREATE TABLE IF NOT EXISTS stage_Sales (
            TransactionID VARCHAR(25) PRIMARY KEY,
            CustomerID VARCHAR(25),
            StoreID VARCHAR(25),
            ProductID VARCHAR(25),
            Date DATE,
            Quantity INTEGER,
            TotalAmount FLOAT
        );
    """
}

# Main Execution for Each Table
s3_paths = {
    "Dim_Employee": "/mnt/s3bucket/cleaned/df_Employee_data/*.csv",
    "Dim_Products": "/mnt/s3bucket/cleaned/df_Products_data/*.csv",
    "Dim_WebData": "/mnt/s3bucket/cleaned/df_WebData_data/*.csv",
    "Dim_Store": "/mnt/s3bucket/cleaned/df_Store_data/*.csv",
    "Fact_Sales": "/mnt/s3bucket/cleaned/df_Sales_data/*.csv"
}

for table, schema in table_schemas.items():
    stage_table = f"stage_{table}"
    target_table = f"dim_{table}" if "Dim" in table else f"fact_{table}"
    s3_path = s3_paths[table]
    create_and_process_table(schema, stage_table, target_table, s3_path, redshift_jdbc_url, redshift_user, redshift_password)
